{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# APACHE SPARK EXERCISES"
      ],
      "metadata": {
        "id": "_5-IU76ueupk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DAG • Broadcast Joins • Transformations • Actions •Partitions"
      ],
      "metadata": {
        "id": "l347G7Epe2mQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DOMAIN\n",
        "\n",
        "Ride-Hailing Platform Analytics (Uber / Ola-like)"
      ],
      "metadata": {
        "id": "eWnsr8-Be-3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark=SparkSession.builder\\\n",
        ".appName(\"DAG and Broadcast demo\")\\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "ESXl-NoVfcN7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 1 — RIDES (Large Fact Table)"
      ],
      "metadata": {
        "id": "zWuhd5m6fCQE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xFq102NVetON"
      },
      "outputs": [],
      "source": [
        "rides_data = [\n",
        "(\"R001\",\"U001\",\"Hyderabad\",12.5,240,\"Completed\"),\n",
        "(\"R002\",\"U002\",\"Delhi\",8.2,180,\"Completed\"),\n",
        "(\"R003\",\"U003\",\"Mumbai\",15.0,300,\"Cancelled\"),\n",
        "(\"R004\",\"U004\",\"Bangalore\",5.5,120,\"Completed\"),\n",
        "(\"R005\",\"U005\",\"Hyderabad\",20.0,360,\"Completed\"),\n",
        "(\"R006\",\"U006\",\"Delhi\",25.0,420,\"Completed\"),\n",
        "(\"R007\",\"U007\",\"Mumbai\",7.5,150,\"Completed\"),\n",
        "(\"R008\",\"U008\",\"Bangalore\",18.0,330,\"Completed\"),\n",
        "(\"R009\",\"U009\",\"Delhi\",6.0,140,\"Cancelled\"),\n",
        "(\"R010\",\"U010\",\"Hyderabad\",10.0,200,\"Completed\")\n",
        "]\n",
        "rides_cols = [\n",
        "\"ride_id\",\n",
        "\"user_id\",\n",
        "\"city\",\n",
        "\"distance_km\",\n",
        "\"duration_seconds\",\n",
        "\"status\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rides_df = spark.createDataFrame(rides_data, rides_cols)"
      ],
      "metadata": {
        "id": "KEx6TBN_fXOK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET 2 — CITY SURGE MULTIPLIERS (Small Lookup)"
      ],
      "metadata": {
        "id": "w6ompbIZfn3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "surge_data = [\n",
        "(\"Hyderabad\",1.2),\n",
        "(\"Delhi\",1.5),\n",
        "(\"Mumbai\",1.8),\n",
        "(\"Bangalore\",1.3)\n",
        "]\n",
        "surge_cols = [\"city\",\"surge_multiplier\"]\n",
        "surge_df = spark.createDataFrame(surge_data, surge_cols)"
      ],
      "metadata": {
        "id": "_jP366xSfYCB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE SET 1 — TRANSFORMATIONS vs ACTIONS"
      ],
      "metadata": {
        "id": "B1jylddmgFFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1.1\n",
        "\n",
        "Create a transformation pipeline that:\n",
        "\n",
        "Filters only Completed rides.\n",
        "Selects ride_id , city , distance_km\n",
        "\n",
        "Tasks:\n",
        "Do not trigger any action\n",
        "Explain whether Spark executed anything"
      ],
      "metadata": {
        "id": "2DpdC7rSgMlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "completed_rides_df = rides_df.filter(rides_df.status == \"Completed\")\n",
        "selected_rides_df = completed_rides_df.select(\"ride_id\", \"city\", \"distance_km\")\n",
        "\n",
        "# To verify the schema without triggering an action (for demonstration, but typically this would cause an action)\n",
        "selected_rides_df.printSchema()\n",
        "\n",
        "# No action is triggered, so Spark has only built the execution plan (DAG) but not executed it."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FAwg_AjgAam",
        "outputId": "060f5155-e256-4b24-e8b2-bd41459bb451"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ride_id: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- distance_km: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1.2\n",
        "\n",
        "Trigger a single action on the pipeline.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Identify which line caused execution\n",
        "Explain why previous lines did not execute"
      ],
      "metadata": {
        "id": "LbVKNT52g2lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selected_rides_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ciRBF_Qgw0q",
        "outputId": "9290fc5c-b8d8-44ba-fc9d-248bc91e7e1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-----------+\n",
            "|ride_id|     city|distance_km|\n",
            "+-------+---------+-----------+\n",
            "|   R001|Hyderabad|       12.5|\n",
            "|   R002|    Delhi|        8.2|\n",
            "|   R004|Bangalore|        5.5|\n",
            "|   R005|Hyderabad|       20.0|\n",
            "|   R006|    Delhi|       25.0|\n",
            "|   R007|   Mumbai|        7.5|\n",
            "|   R008|Bangalore|       18.0|\n",
            "|   R010|Hyderabad|       10.0|\n",
            "+-------+---------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Spark, transformations like filter() and select() are lazily evaluated, meaning they only define the execution plan (DAG) and don't perform any computation until an action, such as .show(), .count(), or .collect(), is called. Therefore, the previous lines that defined completed_rides_df and selected_rides_df did not execute until show() was invoked."
      ],
      "metadata": {
        "id": "VTyVh0pqhKQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE SET 2 — DAG & LINEAGE"
      ],
      "metadata": {
        "id": "eDkjKn_KhP8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2.1\n",
        "\n",
        "Create a transformation chain with:\n",
        "\n",
        "Multiple filters\n",
        "A column selection\n",
        "\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "\n",
        "Identify:\n",
        "Logical plan\n",
        "Optimized logical plan\n",
        "Physical plan"
      ],
      "metadata": {
        "id": "365uxz7DhT2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_selected_rides_df = rides_df.filter(\"status == 'Completed'\") \\\n",
        "                                   .filter(\"distance_km > 10\") \\\n",
        "                                   .select(\"ride_id\", \"city\", \"distance_km\", \"status\")\n",
        "\n",
        "filtered_selected_rides_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNpVkJM8hLLy",
        "outputId": "f7dd15dd-2e7c-4eca-b988-df218d632e15"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project ['ride_id, 'city, 'distance_km, 'status]\n",
            "+- Filter (distance_km#3 > cast(10 as double))\n",
            "   +- Filter (status#5 = Completed)\n",
            "      +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, city: string, distance_km: double, status: string\n",
            "Project [ride_id#0, city#2, distance_km#3, status#5]\n",
            "+- Filter (distance_km#3 > cast(10 as double))\n",
            "   +- Filter (status#5 = Completed)\n",
            "      +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [ride_id#0, city#2, distance_km#3, status#5]\n",
            "+- Filter ((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0)))\n",
            "   +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [ride_id#0, city#2, distance_km#3, status#5]\n",
            "+- *(1) Filter ((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0)))\n",
            "   +- *(1) Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The explain(True) command successfully displayed the different phases of Spark's query optimization:\n",
        "\n",
        "Parsed Logical Plan: This is the initial, unoptimized representation of your transformations as you wrote them. You can see two separate Filter operations and a Project (select) operation.\n",
        "\n",
        "Analyzed Logical Plan: Spark resolves all column references and validates the query. In this case, it confirms the data types and ensures all columns exist.\n",
        "\n",
        "Optimized Logical Plan: Spark's Catalyst optimizer applies various rules to improve efficiency. Notice how the two Filter operations (status == 'Completed' and distance_km > 10) have been combined into a single Filter operation with an AND condition. This is an example of predicate pushdown, where filters are applied as early as possible to reduce the amount of data processed.\n",
        "\n",
        "Physical Plan: This is the concrete execution strategy that Spark will use on the cluster. It shows the actual operations Spark will perform, such as scanning the existing RDD, then applying the combined filter, and finally projecting the selected columns. The *(1) indicates a single stage for these operations."
      ],
      "metadata": {
        "id": "5rXSM3Uuht7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2.2\n",
        "\n",
        "Reorder transformations (filter after join vs before join).\n",
        "Tasks:\n",
        "Compare DAGs\n",
        "Identify which plan is more efficient and why"
      ],
      "metadata": {
        "id": "szspOfHuh3kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join first, then filter\n",
        "joined_then_filtered_df = rides_df.join(surge_df, on=\"city\", how=\"inner\") \\\n",
        "                                .filter(\"status == 'Completed'\") \\\n",
        "                                .filter(\"distance_km > 10\")\n",
        "\n",
        "print(\"\\n--- Plan for Join then Filter ---\")\n",
        "joined_then_filtered_df.explain(True)\n",
        "\n",
        "# Filter first, then join\n",
        "filtered_then_joined_df = rides_df.filter(\"status == 'Completed'\") \\\n",
        "                                .filter(\"distance_km > 10\") \\\n",
        "                                .join(surge_df, on=\"city\", how=\"inner\")\n",
        "\n",
        "print(\"\\n--- Plan for Filter then Join ---\")\n",
        "filtered_then_joined_df.explain(True)\n",
        "\n",
        "\n",
        "# Explanation:\n",
        "# When comparing the 'Optimized Logical Plan' and 'Physical Plan' for both scenarios, you'll observe that Spark's Catalyst optimizer often pushes down predicates (filters) as early as possible.\n",
        "# Even if you write 'join then filter', Spark's optimizer will try to reorder operations to 'filter then join' if it's more efficient.\n",
        "\n",
        "# However, explicitly writing 'filter then join' is often clearer and can sometimes provide hints to the optimizer.\n",
        "# The 'Filter then Join' approach is generally more efficient because:\n",
        "# 1. It reduces the size of the 'rides_df' before the join operation, meaning less data is shuffled and processed during the join.\n",
        "# 2. Joins are expensive operations, and operating on smaller datasets minimizes this cost.\n",
        "# Spark's optimizer is very intelligent and might optimize 'join then filter' to effectively become 'filter then join' under the hood (predicate pushdown).\n",
        "# So, while the logical representation might differ, the physical execution plan (especially the optimized one) might end up being very similar for both, showcasing Spark's optimization capabilities."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOI_x-ORhaRV",
        "outputId": "ceac912d-9c0b-453e-b140-1bd174dcc185"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Plan for Join then Filter ---\n",
            "== Parsed Logical Plan ==\n",
            "'Filter ('distance_km > 10)\n",
            "+- Filter (status#5 = Completed)\n",
            "   +- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "      +- Join Inner, (city#2 = city#6)\n",
            "         :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "         +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Filter (distance_km#3 > cast(10 as double))\n",
            "+- Filter (status#5 = Completed)\n",
            "   +- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "      +- Join Inner, (city#2 = city#6)\n",
            "         :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "         +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter (((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0))) AND isnotnull(city#2))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=53]\n",
            "      :     +- Filter (((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0))) AND isnotnull(city#2))\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=54]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n",
            "\n",
            "--- Plan for Filter then Join ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (distance_km#3 > cast(10 as double))\n",
            ":  +- Filter (status#5 = Completed)\n",
            ":     +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter (distance_km#3 > cast(10 as double))\n",
            "   :  +- Filter (status#5 = Completed)\n",
            "   :     +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter (((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0))) AND isnotnull(city#2))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=84]\n",
            "      :     +- Filter (((isnotnull(status#5) AND isnotnull(distance_km#3)) AND ((status#5 = Completed) AND (distance_km#3 > 10.0))) AND isnotnull(city#2))\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=85]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE SET 3 — PARTITIONS & SHUFFLE"
      ],
      "metadata": {
        "id": "1vHIq8NZirZQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.1\n",
        "\n",
        "Check the number of partitions of rides_df .\n",
        "Tasks:\n",
        "Repartition into 4 partitions\n",
        "\n",
        "Coalesce into 1 partition\n",
        "Observe number of output files when writing to Parquet"
      ],
      "metadata": {
        "id": "kvnkDOgBj3e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# 1. Check initial number of partitions of rides_df\n",
        "initial_partitions = rides_df.rdd.getNumPartitions()\n",
        "print(f\"Initial number of partitions for rides_df: {initial_partitions}\")\n",
        "\n",
        "# Define temporary output directories\n",
        "repartitioned_output_path = \"/tmp/rides_repartitioned_4\"\n",
        "coalesced_output_path = \"/tmp/rides_coalesced_1\"\n",
        "\n",
        "# Clean up previous runs if any\n",
        "if os.path.exists(repartitioned_output_path):\n",
        "    shutil.rmtree(repartitioned_output_path)\n",
        "if os.path.exists(coalesced_output_path):\n",
        "    shutil.rmtree(coalesced_output_path)\n",
        "\n",
        "# 2. Repartition into 4 partitions\n",
        "repartitioned_rides_df = rides_df.repartition(4)\n",
        "repartitioned_partitions = repartitioned_rides_df.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions after repartitioning to 4: {repartitioned_partitions}\")\n",
        "\n",
        "# Write to Parquet and observe output files\n",
        "repartitioned_rides_df.write.mode(\"overwrite\").parquet(repartitioned_output_path)\n",
        "repartitioned_files = [f for f in os.listdir(repartitioned_output_path) if f.endswith('.parquet')]\n",
        "print(f\"Number of output Parquet files after repartitioning: {len(repartitioned_files)}\")\n",
        "\n",
        "# 3. Coalesce into 1 partition\n",
        "coalesced_rides_df = rides_df.coalesce(1)\n",
        "coalesced_partitions = coalesced_rides_df.rdd.getNumPartitions()\n",
        "print(f\"Number of partitions after coalescing to 1: {coalesced_partitions}\")\n",
        "\n",
        "# Write to Parquet and observe output files\n",
        "coalesced_rides_df.write.mode(\"overwrite\").parquet(coalesced_output_path)\n",
        "coalesced_files = [f for f in os.listdir(coalesced_output_path) if f.endswith('.parquet')]\n",
        "print(f\"Number of output Parquet files after coalescing: {len(coalesced_files)}\")\n",
        "\n",
        "# Clean up temporary directories\n",
        "shutil.rmtree(repartitioned_output_path)\n",
        "shutil.rmtree(coalesced_output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAFuCN8viMhO",
        "outputId": "c88efd3f-e410-41c3-fd44-783bfc5fda1e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial number of partitions for rides_df: 2\n",
            "Number of partitions after repartitioning to 4: 4\n",
            "Number of output Parquet files after repartitioning: 4\n",
            "Number of partitions after coalescing to 1: 1\n",
            "Number of output Parquet files after coalescing: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3.2\n",
        "\n",
        "Repartition rides by city .\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify whether a shuffle is introduced"
      ],
      "metadata": {
        "id": "YUAYwjagkQJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repartitioned_rides_by_city_df = rides_df.repartition(\"city\")\n",
        "repartitioned_rides_by_city_df.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b5mDrickRXk",
        "outputId": "bf76dfda-6da7-4dfc-b3b8-1167d0a2749d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'RepartitionByExpression ['city]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "ride_id: string, user_id: string, city: string, distance_km: double, duration_seconds: bigint, status: string\n",
            "RepartitionByExpression [city#2]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "RepartitionByExpression [city#2]\n",
            "+- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Exchange hashpartitioning(city#2, 200), REPARTITION_BY_COL, [plan_id=182]\n",
            "   +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE SET 4 — JOIN WITHOUT BROADCAST (BAD DAG)"
      ],
      "metadata": {
        "id": "dfm8IbAlkYHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4.1\n",
        "\n",
        "Join rides_df with surge_df on city without using broadcast.\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Join type\n",
        "Exchange operators\n",
        "Sort operations\n",
        "Stage boundaries"
      ],
      "metadata": {
        "id": "WozIoy6Ekg33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joined_df_no_broadcast = rides_df.join(surge_df, on=\"city\", how=\"inner\")\n",
        "joined_df_no_broadcast.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZB10DI7kZaq",
        "outputId": "b24f9e1e-3203-442b-eb1f-e34a786c2f36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter isnotnull(city#2)\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=209]\n",
            "      :     +- Filter isnotnull(city#2)\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=210]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4.2\n",
        "\n",
        "Apply a filter ( distance_km > 10 ) before the join.\n",
        "Tasks:\n",
        "Observe whether shuffle is removed\n",
        "Explain why or why not"
      ],
      "metadata": {
        "id": "1g99iwhWksG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_rides_df = rides_df.filter(\"distance_km > 10\")\n",
        "joined_df_filtered_before_join = filtered_rides_df.join(surge_df, on=\"city\", how=\"inner\")\n",
        "\n",
        "print(\"\\n--- Plan for Filter before Join ---\")\n",
        "joined_df_filtered_before_join.explain(True)\n",
        "\n",
        "# Explanation:\n",
        "# Even with the filter applied before the join, the Physical Plan still shows SortMergeJoin with Exchange hashpartitioning operations.\n",
        "# This is because the small size of 'surge_df' (our lookup table) does not automatically trigger a Broadcast Hash Join by Spark's optimizer\n",
        "# in this specific setup, even though it would be more efficient for small lookup tables.\n",
        "# The filter on 'rides_df' only reduces the amount of data processed *before* the shuffle and sort for the join, but it does not eliminate the need for the shuffle itself\n",
        "# for a SortMergeJoin, which requires both sides to be co-located and sorted by the join key.\n",
        "# To explicitly remove the shuffle for the smaller DataFrame, we would need to use a broadcast hint or configure Spark to broadcast smaller tables automatically."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igW6mfTMku0R",
        "outputId": "cfc23e45-3ec2-4e4a-9e1b-f6c144f40757"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Plan for Filter before Join ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- Filter (distance_km#3 > cast(10 as double))\n",
            ":  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter (distance_km#3 > cast(10 as double))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- SortMergeJoin [city#2], [city#6], Inner\n",
            "      :- Sort [city#2 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(city#2, 200), ENSURE_REQUIREMENTS, [plan_id=240]\n",
            "      :     +- Filter ((isnotnull(distance_km#3) AND (distance_km#3 > 10.0)) AND isnotnull(city#2))\n",
            "      :        +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- Sort [city#6 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(city#6, 200), ENSURE_REQUIREMENTS, [plan_id=241]\n",
            "            +- Filter isnotnull(city#6)\n",
            "               +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE SET 5 — BROADCAST JOIN (GOOD DAG)"
      ],
      "metadata": {
        "id": "P25LJf6MlMfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5.1\n",
        "\n",
        "Apply a broadcast hint to surge_df .\n",
        "Tasks:\n",
        "Run explain(True)\n",
        "Identify:\n",
        "Join type\n",
        "BroadcastExchange\n",
        "Disappearance of shuffles"
      ],
      "metadata": {
        "id": "UT0BcIE5lP77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "joined_df_broadcast = rides_df.join(broadcast(surge_df), on=\"city\", how=\"inner\")\n",
        "\n",
        "print(\"\\n--- Plan for Broadcast Join ---\")\n",
        "joined_df_broadcast.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5gEDykWlOG1",
        "outputId": "e3f7fb68-f872-4efc-bf20-3984fcbc51cc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Plan for Broadcast Join ---\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(Inner, [city])\n",
            ":- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "city: string, ride_id: string, user_id: string, distance_km: double, duration_seconds: bigint, status: string, surge_multiplier: double\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6)\n",
            "   :- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "+- Join Inner, (city#2 = city#6), rightHint=(strategy=broadcast)\n",
            "   :- Filter isnotnull(city#2)\n",
            "   :  +- LogicalRDD [ride_id#0, user_id#1, city#2, distance_km#3, duration_seconds#4L, status#5], false\n",
            "   +- Filter isnotnull(city#6)\n",
            "      +- LogicalRDD [city#6, surge_multiplier#7], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [city#2, ride_id#0, user_id#1, distance_km#3, duration_seconds#4L, status#5, surge_multiplier#7]\n",
            "   +- BroadcastHashJoin [city#2], [city#6], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(city#2)\n",
            "      :  +- Scan ExistingRDD[ride_id#0,user_id#1,city#2,distance_km#3,duration_seconds#4L,status#5]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=270]\n",
            "         +- Filter isnotnull(city#6)\n",
            "            +- Scan ExistingRDD[city#6,surge_multiplier#7]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE SET 6 — DAG INTERPRETATION"
      ],
      "metadata": {
        "id": "uK5ceMFgm0bY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6.1\n",
        "\n",
        "From the physical plan:\n",
        "Identify all expensive operators\n",
        "Classify them as CPU, memory, or network heavy"
      ],
      "metadata": {
        "id": "BOxn-HJCrLiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common Expensive Operators and their Resource Classification:\n",
        "\n",
        "# 1. Shuffle operations (e.g., 'Exchange HashPartitioning', 'Exchange Sort')\n",
        "#    - Resource Heavy: Primarily Network and Disk I/O.\n",
        "#    - Explanation: Involve moving data between executors/nodes. Data is serialized, sent over the network, deserialized, and potentially written to disk (shuffle spill). Highly impactful on performance for large datasets.\n",
        "\n",
        "# 2. Sort operations ('Sort')\n",
        "#    - Resource Heavy: Primarily CPU and Memory, potentially Disk I/O.\n",
        "#    - Explanation: Require comparing and ordering data. If data fits in memory, it's CPU/memory-intensive. If not, it spills to disk, becoming Disk I/O heavy.\n",
        "\n",
        "# 3. Joins (e.g., 'SortMergeJoin', 'ShuffleHashJoin', 'BroadcastHashJoin')\n",
        "#    - Resource Heavy: Varies by type.\n",
        "#        - SortMergeJoin: Heavily Network, Disk I/O (due to shuffles and sorts), CPU.\n",
        "#        - ShuffleHashJoin: Heavily Network, Disk I/O (due to shuffles for both tables), Memory/CPU.\n",
        "#        - BroadcastHashJoin: Primarily Memory (for broadcasting small table) and Network (for initial broadcast). Very CPU-efficient for lookups once broadcasted.\n",
        "#    - Explanation: Joins often involve shuffles and comparisons. The most resource-intensive aspects typically come from the shuffling and sorting phases.\n",
        "\n",
        "# 4. Aggregations (e.g., 'HashAggregate', 'SortAggregate')\n",
        "#    - Resource Heavy: Primarily CPU and Memory, potentially Disk I/O.\n",
        "#    - Explanation: Involve grouping data and performing calculations. Hash aggregations use memory for hash tables. Sort aggregations involve sorting. If intermediate data is too large, it spills to disk.\n",
        "\n",
        "# 5. Window functions ('Window')\n",
        "#    - Resource Heavy: Primarily Memory, CPU, and potentially Disk I/O.\n",
        "#    - Explanation: Often require partitioning and sorting data, and holding a portion of data in memory for computations within a 'window'. Can be very memory-intensive for large windows.\n",
        "\n",
        "# Identifying these operators in a physical plan is key to understanding and optimizing Spark application performance."
      ],
      "metadata": {
        "id": "gdTP7OTmm1xY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 7.2\n",
        "\n",
        "Trigger different actions ( count , show , write ) separately.\n",
        "Tasks:\n",
        "Observe whether Spark recomputes the DAG\n",
        "Explain behavior"
      ],
      "metadata": {
        "id": "p9_leWWEryY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# The DataFrame `filtered_selected_rides_df` was defined in Exercise 2.1\n",
        "# It includes multiple filters and a selection.\n",
        "\n",
        "# --- Action 1: count() ---\n",
        "print(\"\\n--- Triggering count() ---\")\n",
        "row_count = filtered_selected_rides_df.count()\n",
        "print(f\"Row count: {row_count}\")\n",
        "# Explanation: Spark executed the entire DAG up to this point to compute the count.\n",
        "\n",
        "# --- Action 2: show() ---\n",
        "print(\"\\n--- Triggering show() ---\")\n",
        "filtered_selected_rides_df.show()\n",
        "# Explanation: Spark re-executed the entire DAG to display the results.\n",
        "\n",
        "# --- Action 3: write() ---\n",
        "output_path = \"/tmp/filtered_rides_output\"\n",
        "# Clean up previous runs if any\n",
        "if os.path.exists(output_path):\n",
        "    shutil.rmtree(output_path)\n",
        "\n",
        "print(\"\\n--- Triggering write() ---\")\n",
        "filtered_selected_rides_df.write.mode(\"overwrite\").parquet(output_path)\n",
        "print(f\"DataFrame written to {output_path}\")\n",
        "# Explanation: Spark re-executed the entire DAG again to write the data.\n",
        "\n",
        "# Clean up temporary directory\n",
        "shutil.rmtree(output_path)\n",
        "\n",
        "# Behavior Explanation:\n",
        "# Spark's operations are lazy. This means that when you define a series of transformations (like filter, select),\n",
        "# Spark doesn't immediately compute the results. Instead, it builds a Directed Acyclic Graph (DAG) of operations.\n",
        "# The actual computation only kicks off when an 'action' is called (like count(), show(), collect(), write()).\n",
        "\n",
        "# Crucially, for each action, Spark, by default, will re-evaluate and re-execute the *entire* DAG from the source\n",
        "# DataFrame. This is why you see the '--- Triggering X() ---' messages followed by output, and each output\n",
        "# corresponds to a full re-computation of the transformations defined on 'filtered_selected_rides_df'.\n",
        "\n",
        "# This behavior can lead to inefficiencies if you perform multiple actions on the same transformed DataFrame.\n",
        "# To avoid recomputing the same transformations repeatedly, you would typically use caching (e.g., .cache() or .persist())\n",
        "# on intermediate DataFrames that are going to be used in multiple subsequent actions."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNLxInohrz5k",
        "outputId": "80211f3b-48d9-49d2-a60e-e36241a10d5b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Triggering count() ---\n",
            "Row count: 4\n",
            "\n",
            "--- Triggering show() ---\n",
            "+-------+---------+-----------+---------+\n",
            "|ride_id|     city|distance_km|   status|\n",
            "+-------+---------+-----------+---------+\n",
            "|   R001|Hyderabad|       12.5|Completed|\n",
            "|   R005|Hyderabad|       20.0|Completed|\n",
            "|   R006|    Delhi|       25.0|Completed|\n",
            "|   R008|Bangalore|       18.0|Completed|\n",
            "+-------+---------+-----------+---------+\n",
            "\n",
            "\n",
            "--- Triggering write() ---\n",
            "DataFrame written to /tmp/filtered_rides_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE SET 8 — THINKING QUESTIONS (WRITTEN)"
      ],
      "metadata": {
        "id": "cNjWZwIur-Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Why does broadcast remove shuffle from the DAG?"
      ],
      "metadata": {
        "id": "875HsFrpsP0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Broadcast joins remove shuffles from the DAG primarily because of how they handle the smaller of the two DataFrames involved in the join:\n",
        "\n",
        "Replication, Not Redistribution: Instead of shuffling both large and small DataFrames to ensure co-located join keys, a broadcast join works by collecting the entire small DataFrame (or table) on the driver and then broadcasting (sending) it to all executor nodes.\n",
        "\n",
        "Local Hash Map: Each executor receives a full copy of the small DataFrame and stores it in memory, typically as a hash map. This makes the data available locally on every node that will process the larger DataFrame.\n",
        "\n",
        "Local Lookups: When the larger DataFrame is processed, each partition of the large DataFrame can perform a local lookup against the in-memory hash map of the broadcasted (smaller) DataFrame. This means there's no need to exchange data between executors to find matching keys.\n",
        "\n",
        "Elimination of Network I/O and Disk I/O: Because the smaller table is already present on each executor, the expensive network transfer (shuffle) and potential disk I/O (if data spills during shuffle/sort) associated with redistributing the large DataFrame's data based on join keys are completely avoided. The join operation transforms from a distributed, shuffle-heavy process into a series of highly efficient local hash lookups.\n",
        "\n",
        "In essence, by proactively moving the small table to every processing unit, Spark eliminates the need for the larger table to be reorganized (shuffled) across the network, thereby removing the shuffle stage from the DAG and significantly improving performance for joins involving a small and a large table."
      ],
      "metadata": {
        "id": "Zm1zNT2HsVva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Why does repartition always introduce shuffle?"
      ],
      "metadata": {
        "id": "YXNcoT5zse7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "repartition() always introduces a shuffle because its primary purpose is to change the number of partitions or the partitioning scheme of a DataFrame. To achieve this, Spark must redistribute the data across the cluster.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "Distributed Nature: Data in Spark DataFrames is distributed across various nodes and executors in a cluster, stored in partitions. Each partition is processed independently.\n",
        "\n",
        "Changing Partition Boundaries: When you call repartition(N) (to change the number of partitions to N) or repartition('column_name') (to repartition by a specific column), Spark needs to ensure that the data is correctly distributed according to the new partitioning logic.\n",
        "\n",
        "Data Movement: To achieve the new distribution, data often needs to move from one physical machine or executor to another. For example, if a row was in partition X on machine A and, after repartitioning, needs to be in partition Y on machine B, it must be sent over the network.\n",
        "\n",
        "Shuffle Mechanism: This movement and redistribution of data across the network (and potentially disk) is precisely what a \"shuffle\" operation is in Spark. Spark uses hashing or sorting mechanisms to determine which new partition each row should go into and then performs the data transfer.\n",
        "\n",
        "coalesce() vs. repartition(): While coalesce() can sometimes avoid a full shuffle if it's only reducing the number of partitions by merging existing ones on the same machines, repartition() guarantees a full shuffle because it rebuilds the partitions from scratch according to the specified number or column, ensuring an even distribution and potentially requiring data to move between all nodes."
      ],
      "metadata": {
        "id": "2mFvk-fhsjkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Why is coalesce cheaper than repartition?"
      ],
      "metadata": {
        "id": "otZW3_FIstx-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d88d9fa"
      },
      "source": [
        "`coalesce()` is generally cheaper than `repartition()` primarily because of how each operation handles data movement across the cluster.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "1.  **Purpose**: Both `coalesce()` and `repartition()` are used to change the number of partitions in a DataFrame.\n",
        "\n",
        "2.  **`repartition()` (Always Shuffles)**:\n",
        "    *   **Goal**: To change the number of partitions to a specified amount and ensure an **even distribution** of data across these new partitions.\n",
        "    *   **Mechanism**: `repartition()` *always* performs a full shuffle of the data. This means all data is read from the existing partitions, then redistributed across the cluster to the new set of partitions according to a hashing or range partitioning strategy. This ensures that the new partitions are roughly equal in size.\n",
        "    *   **Cost**: Full shuffle implies significant network I/O (moving data between nodes), disk I/O (if data spills to disk during shuffle), and CPU overhead (for serialization/deserialization and hashing).\n",
        "\n",
        "3.  **`coalesce()` (Minimizes Shuffle)**:\n",
        "    *   **Goal**: To reduce the number of partitions to a specified amount, while **minimizing data movement**.\n",
        "    *   **Mechanism**: `coalesce()` attempts to achieve the new, smaller number of partitions by merging existing partitions on the same physical nodes. It avoids a full shuffle by trying to combine partitions that are already co-located. If you reduce the number of partitions from `M` to `N` (where `N < M`), `coalesce()` will only move data to `N` nodes (or combine partitions within existing nodes) instead of redistributing all data across `N` new partitions.\n",
        "    *   **Cost**: This approach is much cheaper because it avoids the full-scale data redistribution (shuffle) inherent in `repartition()`. It can lead to imbalanced partitions (some partitions might end up with more data than others) if the initial data distribution is highly skewed, but the trade-off is often worth it for the reduced cost.\n",
        "\n",
        "**In summary**:\n",
        "\n",
        "*   `repartition()` is like starting fresh with new partitions, ensuring even distribution but always incurring the cost of a full shuffle.\n",
        "*   `coalesce()` is like merging existing partitions, minimizing data movement and thus being cheaper, but potentially leading to uneven partition sizes.\n",
        "\n",
        "Therefore, if your goal is only to reduce the number of partitions and you are willing to accept potentially uneven partition sizes, `coalesce()` is the more efficient and cheaper option."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Why does Spark delay execution until an action?"
      ],
      "metadata": {
        "id": "_TY7CXzotBPb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c83683c6"
      },
      "source": [
        "Spark delays execution until an action primarily due to its lazy evaluation model. This design choice offers several significant advantages:\n",
        "\n",
        "1.  **Optimization through DAGs (Directed Acyclic Graphs)**:\n",
        "    *   When you define transformations (like `filter()`, `select()`, `join()`), Spark doesn't immediately compute the results. Instead, it builds a logical plan, represented as a DAG, of all the operations you want to perform.\n",
        "    *   Only when an action (like `show()`, `count()`, `collect()`, `write()`) is triggered does Spark's Catalyst optimizer analyze this entire DAG. It can then optimize the sequence of operations, combine transformations, eliminate unnecessary steps (e.g., predicate pushdown, column pruning), and choose the most efficient physical execution plan.\n",
        "    *   If Spark executed every transformation immediately, it wouldn't have the holistic view of the entire pipeline needed for these global optimizations.\n",
        "\n",
        "2.  **Efficiency and Resource Management**:\n",
        "    *   **Reduced I/O**: By knowing the full lineage of operations, Spark can minimize data reads and writes. For example, if you filter a DataFrame and then select only a few columns, Spark can optimize to read only the necessary columns from the start and apply the filter early, avoiding reading and processing data that will eventually be discarded.\n",
        "    *   **Avoiding Intermediate Materialization**: Spark avoids creating unnecessary intermediate DataFrames or RDDs in memory or on disk. This saves storage space and network I/O.\n",
        "    *   **Fault Tolerance**: Lazy evaluation is key to Spark's fault tolerance. If a node fails during execution, Spark can re-execute only the lost partitions of the DAG from the last checkpoint or source data, rather than recomputing the entire dataset from scratch.\n",
        "\n",
        "3.  **Conciseness and Expressiveness**: It allows developers to write complex data transformation pipelines in a more declarative and readable way, focusing on *what* they want to achieve rather than *how* to execute it step-by-step.\n",
        "\n",
        "In summary, lazy evaluation enables Spark to perform powerful query optimizations, manage resources more efficiently, and provide robust fault tolerance, all of which contribute to its performance and scalability for big data processing."
      ]
    }
  ]
}