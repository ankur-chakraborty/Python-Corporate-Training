{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1 DATA"
      ],
      "metadata": {
        "id": "QGLU90gGAzVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark=SparkSession.builder.appName('Struct Type').getOrCreate()\n",
        "\n",
        "raw_users = [\n",
        "(\"U001\",\"Amit\",\"28\",\"Hyderabad\",\"['AI','ML','Cloud']\"),\n",
        "(\"U002\",\"Neha\",\"Thirty\",\"Delhi\",\"AI,Testing\"),\n",
        "(\"U003\",\"Ravi\",None,\"Bangalore\",[\"Data\",\"Spark\"]),\n",
        "(\"U004\",\"Pooja\",\"29\",\"Mumbai\",None),\n",
        "(\"U005\",\"\", \"31\",\"Chennai\",\"['DevOps']\")\n",
        "]\n",
        ""
      ],
      "metadata": {
        "id": "GNODI7_BAyzy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import(\n",
        "    StructType,\n",
        "    StructField,\n",
        "    StringType,\n",
        "    IntegerType,\n",
        "    LongType\n",
        ")\n",
        "\n",
        "users_schema=StructType([\n",
        "    StructField(\"user_id\",StringType(),True),\n",
        "    StructField(\"name\",StringType(),True),\n",
        "    StructField(\"age\",StringType(),True),\n",
        "    StructField(\"city\",StringType(),True),\n",
        "    StructField(\"interests\",StringType(),True)\n",
        "])"
      ],
      "metadata": {
        "id": "b2Zb9qkCB6Yf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,when,regexp_replace\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_users=spark.createDataFrame(raw_users,users_schema)\n",
        "df_users.show()\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_users_age_normalized = df_users.withColumn(\n",
        "    \"age\",\n",
        "    when(col(\"age\").rlike(\"^[0-9]+$\"), col(\"age\").cast(IntegerType()))\n",
        "    .otherwise(None)\n",
        ")\n",
        "\n",
        "df_users_age_normalized.show()\n",
        "df_users_age_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDZVx9jwCCRP",
        "outputId": "ef8201d1-3062-415c-e45e-a5a05dc7adce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+------+---------+-------------------+\n",
            "|user_id| name|   age|     city|          interests|\n",
            "+-------+-----+------+---------+-------------------+\n",
            "|   U001| Amit|    28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha|Thirty|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|  NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|    29|   Mumbai|               NULL|\n",
            "|   U005|     |    31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+------+---------+-------------------+\n",
            "\n",
            "+-------+-----+----+---------+-------------------+\n",
            "|user_id| name| age|     city|          interests|\n",
            "+-------+-----+----+---------+-------------------+\n",
            "|   U001| Amit|  28|Hyderabad|['AI','ML','Cloud']|\n",
            "|   U002| Neha|NULL|    Delhi|         AI,Testing|\n",
            "|   U003| Ravi|NULL|Bangalore|      [Data, Spark]|\n",
            "|   U004|Pooja|  29|   Mumbai|               NULL|\n",
            "|   U005|     |  31|  Chennai|         ['DevOps']|\n",
            "+-------+-----+----+---------+-------------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, split, regexp_replace, when, trim, expr\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "df_users_skills_normalized = df_users_age_normalized.withColumn(\n",
        "    \"cleaned_interests_str\",\n",
        "    when(\n",
        "        col(\"interests\").isNotNull(),\n",
        "        regexp_replace(\n",
        "            regexp_replace(\n",
        "                col(\"interests\"),\n",
        "                \"^\\[|\\]$\", \"\"\n",
        "            ),\n",
        "            \"'\", \"\"\n",
        "        )\n",
        "    ).otherwise(None)\n",
        ").withColumn(\n",
        "    \"interests\",\n",
        "    when(\n",
        "        col(\"cleaned_interests_str\").isNotNull(),\n",
        "        split(col(\"cleaned_interests_str\"), \",\")\n",
        "    ).otherwise(None)\n",
        ").withColumn(\n",
        "    \"interests\",\n",
        "    when(\n",
        "        col(\"interests\").isNotNull(),\n",
        "\n",
        "        expr(\"filter(transform(interests, x -> trim(x)), x -> x != '')\")\n",
        "    ).otherwise(None)\n",
        ").drop(\"cleaned_interests_str\")\n",
        "\n",
        "df_users_skills_normalized.show()\n",
        "df_users_skills_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsVI7s4XB7SX",
        "outputId": "85708431-d736-4544-b7fd-c4cb47e1afb2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:11: SyntaxWarning: invalid escape sequence '\\['\n",
            "<>:11: SyntaxWarning: invalid escape sequence '\\['\n",
            "/tmp/ipython-input-2797853043.py:11: SyntaxWarning: invalid escape sequence '\\['\n",
            "  \"^\\[|\\]$\", \"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+----+---------+---------------+\n",
            "|user_id| name| age|     city|      interests|\n",
            "+-------+-----+----+---------+---------------+\n",
            "|   U001| Amit|  28|Hyderabad|[AI, ML, Cloud]|\n",
            "|   U002| Neha|NULL|    Delhi|  [AI, Testing]|\n",
            "|   U003| Ravi|NULL|Bangalore|  [Data, Spark]|\n",
            "|   U004|Pooja|  29|   Mumbai|           NULL|\n",
            "|   U005|     |  31|  Chennai|       [DevOps]|\n",
            "+-------+-----+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when\n",
        "\n",
        "df_users_names_handled = df_users_skills_normalized.withColumn(\n",
        "    \"name\",\n",
        "    when(col(\"name\").isNull() | (col(\"name\") == \"\"), \"Unknown\")\n",
        "    .otherwise(col(\"name\"))\n",
        ")\n",
        "\n",
        "df_users_names_handled.show()\n",
        "df_users_names_handled.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNWvso3SCP-5",
        "outputId": "e102ad80-b959-4615-ca00-b0c020f33daf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+---------------+\n",
            "|user_id|   name| age|     city|      interests|\n",
            "+-------+-------+----+---------+---------------+\n",
            "|   U001|   Amit|  28|Hyderabad|[AI, ML, Cloud]|\n",
            "|   U002|   Neha|NULL|    Delhi|  [AI, Testing]|\n",
            "|   U003|   Ravi|NULL|Bangalore|  [Data, Spark]|\n",
            "|   U004|  Pooja|  29|   Mumbai|           NULL|\n",
            "|   U005|Unknown|  31|  Chennai|       [DevOps]|\n",
            "+-------+-------+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "users_df = df_users_names_handled\n",
        "\n",
        "users_df.show()\n",
        "users_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKuX2xZRCTYC",
        "outputId": "216480c2-2651-4e37-8ab2-838b86eeef23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----+---------+---------------+\n",
            "|user_id|   name| age|     city|      interests|\n",
            "+-------+-------+----+---------+---------------+\n",
            "|   U001|   Amit|  28|Hyderabad|[AI, ML, Cloud]|\n",
            "|   U002|   Neha|NULL|    Delhi|  [AI, Testing]|\n",
            "|   U003|   Ravi|NULL|Bangalore|  [Data, Spark]|\n",
            "|   U004|  Pooja|  29|   Mumbai|           NULL|\n",
            "|   U005|Unknown|  31|  Chennai|       [DevOps]|\n",
            "+-------+-------+----+---------+---------------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- interests: array (nullable = true)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2 Data"
      ],
      "metadata": {
        "id": "J7e3_AcfCYLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_courses = [\n",
        "(\"C001\",\"PySpark Mastery\",(\"Data Engineering\",\"Advanced\"),\"₹9999\"),\n",
        "(\"C002\",\"AI for Testers\",{\"domain\":\"QA\",\"level\":\"Beginner\"},\"8999\"),\n",
        "(\"C003\",\"ML Foundations\",(\"AI\",\"Intermediate\"),None),\n",
        "(\"C004\",\"Data Engineering Bootcamp\",\"Data|Advanced\",\"₹14999\")\n",
        "]"
      ],
      "metadata": {
        "id": "VA7yx8LHCZ8R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType\n",
        "\n",
        "course_metadata_schema = StructType([\n",
        "    StructField(\"domain\", StringType(), True),\n",
        "    StructField(\"level\", StringType(), True)\n",
        "])\n",
        "\n",
        "course_schema = StructType([\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"title\", StringType(), True),\n",
        "    StructField(\"metadata\", course_metadata_schema, True),\n",
        "    StructField(\"price\", StringType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "htYetYc2CdcZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, split, struct, lit\n",
        "from pyspark.sql.types import StringType, StructType, StructField, Row\n",
        "\n",
        "processed_raw_courses = []\n",
        "for course_id, title, metadata_raw, price in raw_courses:\n",
        "    domain = None\n",
        "    level = None\n",
        "\n",
        "    if isinstance(metadata_raw, tuple) and len(metadata_raw) == 2:\n",
        "        domain = metadata_raw[0]\n",
        "        level = metadata_raw[1]\n",
        "    elif isinstance(metadata_raw, dict):\n",
        "        domain = metadata_raw.get(\"domain\")\n",
        "        level = metadata_raw.get(\"level\")\n",
        "    elif isinstance(metadata_raw, str):\n",
        "        parts = metadata_raw.split('|')\n",
        "        if len(parts) == 2:\n",
        "            domain = parts[0]\n",
        "            level = parts[1]\n",
        "\n",
        "    metadata_struct_row = Row(domain=domain, level=level) if (domain is not None or level is not None) else None\n",
        "\n",
        "    processed_raw_courses.append(Row(course_id=course_id, title=title, metadata=metadata_struct_row, price=price))\n",
        "\n",
        "df_courses_normalized = spark.createDataFrame(processed_raw_courses, course_schema)\n",
        "\n",
        "df_courses_normalized.show(truncate=False)\n",
        "df_courses_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9_0ubPnCiML",
        "outputId": "b5d5ff83-26c4-4edf-d40c-eeb826274d6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------------------+----------------------------+------+\n",
            "|course_id|title                    |metadata                    |price |\n",
            "+---------+-------------------------+----------------------------+------+\n",
            "|C001     |PySpark Mastery          |{Data Engineering, Advanced}|₹9999 |\n",
            "|C002     |AI for Testers           |{QA, Beginner}              |8999  |\n",
            "|C003     |ML Foundations           |{AI, Intermediate}          |NULL  |\n",
            "|C004     |Data Engineering Bootcamp|{Data, Advanced}            |₹14999|\n",
            "+---------+-------------------------+----------------------------+------+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, regexp_replace, when\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "df_courses_final = df_courses_normalized.withColumn(\n",
        "    \"price\",\n",
        "    when(col(\"price\").isNotNull(),\n",
        "         regexp_replace(col(\"price\"), \"₹\", \"\").cast(IntegerType())\n",
        "    ).otherwise(None)\n",
        ")\n",
        "\n",
        "df_courses_final.show()\n",
        "df_courses_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATB_M4L2DGcN",
        "outputId": "6a5e39ee-bbee-4d03-f673-0ec688ca6ec3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+-----+\n",
            "|course_id|               title|            metadata|price|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "|     C001|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C003|      ML Foundations|  {AI, Intermediate}| NULL|\n",
            "|     C004|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "df_courses_final = df_courses_final.withColumn(\n",
        "    \"price\",\n",
        "    when(col(\"price\").isNull(), 0).otherwise(col(\"price\"))\n",
        ")\n",
        "\n",
        "\n",
        "df_courses_final.show()\n",
        "df_courses_final.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIBZfNz_DLyf",
        "outputId": "ba5462bc-c06b-45b7-a3cb-6ebf355a7b80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+-----+\n",
            "|course_id|               title|            metadata|price|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "|     C001|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C003|      ML Foundations|  {AI, Intermediate}|    0|\n",
            "|     C004|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "courses_df = df_courses_final\n",
        "\n",
        "courses_df.show()\n",
        "courses_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPP6lQz2DR7P",
        "outputId": "843de836-aa63-4c44-f00c-f0d8efc7a2be"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------------------+--------------------+-----+\n",
            "|course_id|               title|            metadata|price|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "|     C001|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C003|      ML Foundations|  {AI, Intermediate}|    0|\n",
            "|     C004|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+--------------------+--------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 DATASET"
      ],
      "metadata": {
        "id": "cmlnHfS5DT1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_enrollments = [\n",
        "(\"U001\",\"C001\",\"2024-01-05\"),\n",
        "(\"U002\",\"C002\",\"05/01/2024\"),\n",
        "(\"U003\",\"C001\",\"2024/01/06\"),\n",
        "(\"U004\",\"C003\",\"invalid_date\"),\n",
        "(\"U001\",\"C004\",\"2024-01-10\")\n",
        "]\n",
        "\n",
        "enroll_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"course_id\", StringType(), True),\n",
        "    StructField(\"enrollment_date_raw\", StringType(), True)\n",
        "])\n",
        "df_enrollments_raw = spark.createDataFrame(raw_enrollments, enroll_schema)\n",
        "df_enrollments_raw.printSchema()\n",
        "df_enrollments_raw.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yvKhZ35DWBe",
        "outputId": "22413f6a-37cf-4c39-c528-74ac9474d014"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- enrollment_date_raw: string (nullable = true)\n",
            "\n",
            "+-------+---------+-------------------+\n",
            "|user_id|course_id|enrollment_date_raw|\n",
            "+-------+---------+-------------------+\n",
            "|   U001|     C001|         2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|\n",
            "|   U003|     C001|         2024/01/06|\n",
            "|   U004|     C003|       invalid_date|\n",
            "|   U001|     C004|         2024-01-10|\n",
            "+-------+---------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import coalesce, col, when, broadcast, to_date\n",
        "\n",
        "df_enrollments_raw = df_enrollments_raw.withColumn(\n",
        "    \"enrollment_date\",\n",
        "    coalesce(\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{4}-\\\\d{2}-\\\\d{2}$|^\\\\d{4}-\\\\d{2}-\\\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy-MM-dd\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{2}/\\\\d{2}/\\\\d{4}$|^\\\\d{2}/\\\\d{2}/\\\\d{4}$\"), to_date(col(\"enrollment_date_raw\"), \"dd/MM/yyyy\")),\n",
        "        when(col(\"enrollment_date_raw\").rlike(\"^\\\\d{4}/\\\\d{2}/\\\\d{2}$|^\\\\d{4}/\\\\d{2}/\\\\d{2}$\"), to_date(col(\"enrollment_date_raw\"), \"yyyy/MM/dd\"))\n",
        "    )\n",
        ")\n",
        "df_enrollments_raw.show()\n",
        "\n",
        "df_enrollments_processed = df_enrollments_raw.drop(\"enrollment_date_raw\")\n",
        "df_enriched = df_enrollments_processed.join(broadcast(courses_df), on=\"course_id\", how=\"left\")\n",
        "df_enriched.show()\n",
        "\n",
        "df_enriched.show(truncate=False)\n",
        "df_enriched.printSchema()\n",
        "\n",
        "# Decision: Broadcast df_courses_clean\n",
        "# Reasoning: The `df_courses_clean` (course catalog) is expected to be significantly smaller than `df_enrollments_processed` (user enrollments).\n",
        "# Broadcasting the smaller table to all worker nodes during a join optimizes performance by avoiding a shuffle of the larger DataFrame and reducing network I/O.\n",
        "# This was already implemented in the previous join: `df_enrollments_processed.join(broadcast(df_courses_clean), on=\"course_id\", how=\"left\")`\n",
        "\n",
        "df_enriched.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zA0UKQ7GDrhK",
        "outputId": "60de4606-0b95-4834-8aa4-9530d26335bf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+-------------------+---------------+\n",
            "|user_id|course_id|enrollment_date_raw|enrollment_date|\n",
            "+-------+---------+-------------------+---------------+\n",
            "|   U001|     C001|         2024-01-05|     2024-01-05|\n",
            "|   U002|     C002|         05/01/2024|     2024-01-05|\n",
            "|   U003|     C001|         2024/01/06|     2024-01-06|\n",
            "|   U004|     C003|       invalid_date|           NULL|\n",
            "|   U001|     C004|         2024-01-10|     2024-01-10|\n",
            "+-------+---------+-------------------+---------------+\n",
            "\n",
            "+---------+-------+---------------+--------------------+--------------------+-----+\n",
            "|course_id|user_id|enrollment_date|               title|            metadata|price|\n",
            "+---------+-------+---------------+--------------------+--------------------+-----+\n",
            "|     C001|   U001|     2024-01-05|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C002|   U002|     2024-01-05|      AI for Testers|      {QA, Beginner}| 8999|\n",
            "|     C001|   U003|     2024-01-06|     PySpark Mastery|{Data Engineering...| 9999|\n",
            "|     C003|   U004|           NULL|      ML Foundations|  {AI, Intermediate}|    0|\n",
            "|     C004|   U001|     2024-01-10|Data Engineering ...|    {Data, Advanced}|14999|\n",
            "+---------+-------+---------------+--------------------+--------------------+-----+\n",
            "\n",
            "+---------+-------+---------------+-------------------------+----------------------------+-----+\n",
            "|course_id|user_id|enrollment_date|title                    |metadata                    |price|\n",
            "+---------+-------+---------------+-------------------------+----------------------------+-----+\n",
            "|C001     |U001   |2024-01-05     |PySpark Mastery          |{Data Engineering, Advanced}|9999 |\n",
            "|C002     |U002   |2024-01-05     |AI for Testers           |{QA, Beginner}              |8999 |\n",
            "|C001     |U003   |2024-01-06     |PySpark Mastery          |{Data Engineering, Advanced}|9999 |\n",
            "|C003     |U004   |NULL           |ML Foundations           |{AI, Intermediate}          |0    |\n",
            "|C004     |U001   |2024-01-10     |Data Engineering Bootcamp|{Data, Advanced}            |14999|\n",
            "+---------+-------+---------------+-------------------------+----------------------------+-----+\n",
            "\n",
            "root\n",
            " |-- course_id: string (nullable = true)\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- enrollment_date: date (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- metadata: struct (nullable = true)\n",
            " |    |-- domain: string (nullable = true)\n",
            " |    |-- level: string (nullable = true)\n",
            " |-- price: integer (nullable = true)\n",
            "\n",
            "== Parsed Logical Plan ==\n",
            "'Join UsingJoin(LeftOuter, [course_id])\n",
            ":- Project [user_id#242, course_id#243, enrollment_date#297]\n",
            ":  +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#297]\n",
            ":     +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$|^\\d{2}/\\d{2}/\\d{4}$|^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#283]\n",
            ":        +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$|^\\d{2}/\\d{2}/\\d{4}$|\\d{4}-\\d{2}-\\d{2}) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#269]\n",
            ":           +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#255]\n",
            ":              +- LogicalRDD [user_id#242, course_id#243, enrollment_date_raw#244], false\n",
            "+- ResolvedHint (strategy=broadcast)\n",
            "   +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnull(price#201) THEN 0 ELSE price#201 END AS price#215]\n",
            "      +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnotnull(price#187) THEN cast(regexp_replace(price#187, ₹, , 1) as int) ELSE cast(null as int) END AS price#201]\n",
            "         +- LogicalRDD [course_id#184, title#185, metadata#186, price#187], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "course_id: string, user_id: string, enrollment_date: date, title: string, metadata: struct<domain:string,level:string>, price: int\n",
            "Project [course_id#243, user_id#242, enrollment_date#297, title#185, metadata#186, price#215]\n",
            "+- Join LeftOuter, (course_id#243 = course_id#184)\n",
            "   :- Project [user_id#242, course_id#243, enrollment_date#297]\n",
            "   :  +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#297]\n",
            "   :     +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$|^\\d{2}/\\d{2}/\\d{4}$|^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#283]\n",
            "   :        +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$|^\\d{2}/\\d{2}/\\d{4}$|\\d{4}-\\d{2}-\\d{2}) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#269]\n",
            "   :           +- Project [user_id#242, course_id#243, enrollment_date_raw#244, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy-MM-dd), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$) THEN to_date(enrollment_date_raw#244, Some(dd/MM/yyyy), Some(Etc/UTC), true) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$) THEN to_date(enrollment_date_raw#244, Some(yyyy/MM/dd), Some(Etc/UTC), true) END) AS enrollment_date#255]\n",
            "   :              +- LogicalRDD [user_id#242, course_id#243, enrollment_date_raw#244], false\n",
            "   +- ResolvedHint (strategy=broadcast)\n",
            "      +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnull(price#201) THEN 0 ELSE price#201 END AS price#215]\n",
            "         +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnotnull(price#187) THEN cast(regexp_replace(price#187, ₹, , 1) as int) ELSE cast(null as int) END AS price#201]\n",
            "            +- LogicalRDD [course_id#184, title#185, metadata#186, price#187], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [course_id#243, user_id#242, enrollment_date#297, title#185, metadata#186, price#215]\n",
            "+- Join LeftOuter, (course_id#243 = course_id#184), rightHint=(strategy=broadcast)\n",
            "   :- Project [user_id#242, course_id#243, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#244, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#244, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#244, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#297]\n",
            "   :  +- LogicalRDD [user_id#242, course_id#243, enrollment_date_raw#244], false\n",
            "   +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnull(price#201) THEN 0 ELSE price#201 END AS price#215]\n",
            "      +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnotnull(price#187) THEN cast(regexp_replace(price#187, ₹, , 1) as int) END AS price#201]\n",
            "         +- Filter isnotnull(course_id#184)\n",
            "            +- LogicalRDD [course_id#184, title#185, metadata#186, price#187], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [course_id#243, user_id#242, enrollment_date#297, title#185, metadata#186, price#215]\n",
            "   +- BroadcastHashJoin [course_id#243], [course_id#184], LeftOuter, BuildRight, false\n",
            "      :- Project [user_id#242, course_id#243, coalesce(CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}-\\d{2}-\\d{2}$|^\\d{4}-\\d{2}-\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#244, yyyy-MM-dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{2}/\\d{2}/\\d{4}$|^\\d{2}/\\d{2}/\\d{4}$) THEN cast(gettimestamp(enrollment_date_raw#244, dd/MM/yyyy, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END, CASE WHEN RLIKE(enrollment_date_raw#244, ^\\d{4}/\\d{2}/\\d{2}$|^\\d{4}/\\d{2}/\\d{2}$) THEN cast(gettimestamp(enrollment_date_raw#244, yyyy/MM/dd, TimestampType, try_to_timestamp, Some(Etc/UTC), true) as date) END) AS enrollment_date#297]\n",
            "      :  +- Scan ExistingRDD[user_id#242,course_id#243,enrollment_date_raw#244]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=626]\n",
            "         +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnull(price#201) THEN 0 ELSE price#201 END AS price#215]\n",
            "            +- Project [course_id#184, title#185, metadata#186, CASE WHEN isnotnull(price#187) THEN cast(regexp_replace(price#187, ₹, , 1) as int) END AS price#201]\n",
            "               +- Filter isnotnull(course_id#184)\n",
            "                  +- Scan ExistingRDD[course_id#184,title#185,metadata#186,price#187]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 DATASET"
      ],
      "metadata": {
        "id": "p04HO-ykExcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, IntegerType\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, when, split\n",
        "raw_activity = [\n",
        "(\"U001\",\"login,watch,logout\",\"{'device':'mobile','ip':'1.1.1.1'}\",120),\n",
        "(\"U002\",[\"login\",\"watch\"],\"device=laptop;ip=2.2.2.2\",90),\n",
        "(\"U003\",\"login|logout\",None,30),\n",
        "(\"U004\",None,\"{'device':'tablet'}\",60)\n",
        "]\n",
        "# 1. Define the schema for the activity data\n",
        "activity_schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"actions\", ArrayType(StringType()), True), # Normalize actions to ArrayType\n",
        "    StructField(\"properties\", StringType(), True),\n",
        "    StructField(\"duration\", IntegerType(), True)\n",
        "])"
      ],
      "metadata": {
        "id": "9hCVEeOCE2CS"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_raw_activity = []\n",
        "for user_id, actions_raw, properties, duration in raw_activity:\n",
        "    normalized_actions = None\n",
        "    if actions_raw is None:\n",
        "        normalized_actions = None\n",
        "    elif isinstance(actions_raw, list):\n",
        "        normalized_actions = actions_raw\n",
        "    elif isinstance(actions_raw, str):\n",
        "        if ',' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split(',')]\n",
        "        elif '|' in actions_raw:\n",
        "            normalized_actions = [a.strip() for a in actions_raw.split('|')]\n",
        "\n",
        "    processed_raw_activity.append(Row(user_id=user_id, actions=normalized_actions, properties=properties, duration=duration))"
      ],
      "metadata": {
        "id": "kCDhLvtYE7cS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_activity = spark.createDataFrame(processed_raw_activity, activity_schema)\n",
        "\n",
        "# Display the DataFrame and its schema\n",
        "df_activity.show(truncate=False)\n",
        "df_activity.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbgjjiq9FRrb",
        "outputId": "b9723d1d-2106-4674-8df2-348c34e6a2a3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+----------------------------------+--------+\n",
            "|user_id|actions               |properties                        |duration|\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{'device':'mobile','ip':'1.1.1.1'}|120     |\n",
            "|U002   |[login, watch]        |device=laptop;ip=2.2.2.2          |90      |\n",
            "|U003   |[login, logout]       |NULL                              |30      |\n",
            "|U004   |NULL                  |{'device':'tablet'}               |60      |\n",
            "+-------+----------------------+----------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, from_json, regexp_replace, udf\n",
        "from pyspark.sql.types import MapType, StringType\n",
        "\n",
        "# Define a UDF to parse custom key-value strings like \"device=laptop;ip=2.2.2.2\"\n",
        "def parse_custom_properties(s):\n",
        "    if s is None:\n",
        "        return None\n",
        "    try:\n",
        "        parts = s.split(';')\n",
        "        result_map = {}\n",
        "        for part in parts:\n",
        "            if '=' in part:\n",
        "                key, value = part.split('=', 1)\n",
        "                result_map[key.strip()] = value.strip()\n",
        "        return result_map\n",
        "    except Exception:\n",
        "        return None # Return None for malformed strings\n",
        "\n",
        "# Register the UDF\n",
        "parse_custom_properties_udf = udf(parse_custom_properties, MapType(StringType(), StringType()))\n",
        "\n",
        "df_activity_normalized = df_activity.withColumn(\n",
        "    \"properties\",\n",
        "    when(col(\"properties\").isNull(), None)\n",
        "    .when(col(\"properties\").startswith(\"{\"), # Check for JSON-like strings (start with '{')\n",
        "          from_json(regexp_replace(col(\"properties\"), \"'\", \"\\\"\"), MapType(StringType(), StringType())))\n",
        "    .otherwise(parse_custom_properties_udf(col(\"properties\"))) # Handle custom format for others\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvHRXJMQFX38",
        "outputId": "ae34c75e-cb7d-4112-de4d-bee671249db0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |NULL                  |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, array\n",
        "\n",
        "# Handle missing actions safely by replacing NULL with empty array\n",
        "df_activity_normalized = df_activity_normalized.withColumn(\n",
        "    \"actions\",\n",
        "    when(col(\"actions\").isNull(), array()).otherwise(col(\"actions\"))\n",
        ")\n",
        "\n",
        "df_activity_normalized.show(truncate=False)\n",
        "df_activity_normalized.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXUcmanhFs1n",
        "outputId": "1cac8cb7-791c-40b7-e0f0-2e630e1a3d74"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode, col, count\n",
        "\n",
        "# Explode the 'actions' array to create a new row for each action\n",
        "df_exploded_actions = df_activity_normalized.select(col(\"user_id\"), explode(col(\"actions\")).alias(\"action\"))\n",
        "\n",
        "# Count the frequency of each action\n",
        "action_frequency = df_exploded_actions.groupBy(\"action\").agg(count(\"action\").alias(\"frequency\"))\n",
        "\n",
        "# Show the results, ordered by frequency\n",
        "action_frequency.orderBy(col(\"frequency\").desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqD4NFVBF2sW",
        "outputId": "cccc8abb-cc66-444c-ffac-2827cad8bcc3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+\n",
            "|action|frequency|\n",
            "+------+---------+\n",
            "| login|        3|\n",
            "| watch|        2|\n",
            "|logout|        2|\n",
            "+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activity_df = df_activity_normalized\n",
        "\n",
        "activity_df.show(truncate=False)\n",
        "activity_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8Phdwd7F-3H",
        "outputId": "44c13087-18cc-4c5c-929b-4c3ec3344349"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------------+---------------------------------+--------+\n",
            "|user_id|actions               |properties                       |duration|\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "|U001   |[login, watch, logout]|{device -> mobile, ip -> 1.1.1.1}|120     |\n",
            "|U002   |[login, watch]        |{device -> laptop, ip -> 2.2.2.2}|90      |\n",
            "|U003   |[login, logout]       |NULL                             |30      |\n",
            "|U004   |[]                    |{device -> tablet}               |60      |\n",
            "+-------+----------------------+---------------------------------+--------+\n",
            "\n",
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- actions: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            " |-- duration: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5 DATASET"
      ],
      "metadata": {
        "id": "16300A6EAs15"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooSR3cNv2dZJ",
        "outputId": "6d7b2c2f-ff03-47d8-a8c3-fa6b2bf4122a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+\n",
            "|user_id|      date|amount|\n",
            "+-------+----------+------+\n",
            "|   U001|2024-01-05|  9999|\n",
            "|   U001|2024-01-10| 14999|\n",
            "|   U002|2024-01-06|  8999|\n",
            "|   U003|2024-01-07|     0|\n",
            "|   U004|2024-01-08|  7999|\n",
            "|   U001|2024-01-15|  1999|\n",
            "+-------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import sum\n",
        "spark=SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "(\"U001\",\"2024-01-05\",9999),\n",
        "(\"U001\",\"2024-01-10\",14999),\n",
        "(\"U002\",\"2024-01-06\",8999),\n",
        "(\"U003\",\"2024-01-07\",0),\n",
        "(\"U004\",\"2024-01-08\",7999),\n",
        "(\"U001\",\"2024-01-15\",1999)\n",
        "]\n",
        "\n",
        "columns = [\"user_id\",\"date\",\"amount\"]\n",
        "\n",
        "df=spark.createDataFrame(data,columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "df = df.withColumn(\"date\", to_date(df[\"date\"], \"yyyy-MM-dd\"))\n",
        "\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dns6s_Ma-dh5",
        "outputId": "dd9f14d8-c26c-4109-a3e9-02c543ac0422"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: string (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            " |-- amount: long (nullable = true)\n",
            "\n",
            "+-------+----------+------+\n",
            "|user_id|      date|amount|\n",
            "+-------+----------+------+\n",
            "|   U001|2024-01-05|  9999|\n",
            "|   U001|2024-01-10| 14999|\n",
            "|   U002|2024-01-06|  8999|\n",
            "|   U003|2024-01-07|     0|\n",
            "|   U004|2024-01-08|  7999|\n",
            "|   U001|2024-01-15|  1999|\n",
            "+-------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_spend_per_user = df.groupBy(\"user_id\").sum(\"amount\")\n",
        "total_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7JdAj1c-xb6",
        "outputId": "ee8a5eff-0ee1-40b7-ee78-64ac6289ea09"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|user_id|sum(amount)|\n",
            "+-------+-----------+\n",
            "|   U002|       8999|\n",
            "|   U001|      26997|\n",
            "|   U004|       7999|\n",
            "|   U003|          0|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = Window.partitionBy(\"user_id\").orderBy(\"date\")\n",
        "running_spend_per_user = df.withColumn(\"running_spend\", sum(\"amount\").over(window_spec))\n",
        "running_spend_per_user.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zquoe3-_A6L",
        "outputId": "536e3ba3-8df1-4cfc-e623-d12db02f8952"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+------+-------------+\n",
            "|user_id|      date|amount|running_spend|\n",
            "+-------+----------+------+-------------+\n",
            "|   U001|2024-01-05|  9999|         9999|\n",
            "|   U001|2024-01-10| 14999|        24998|\n",
            "|   U001|2024-01-15|  1999|        26997|\n",
            "|   U002|2024-01-06|  8999|         8999|\n",
            "|   U003|2024-01-07|     0|            0|\n",
            "|   U004|2024-01-08|  7999|         7999|\n",
            "+-------+----------+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window_spec_rank = Window.orderBy(desc(\"sum(amount)\"))\n",
        "\n",
        "ranked_users_by_total_spend = total_spend_per_user.withColumn(\"rank\", rank().over(window_spec_rank))\n",
        "\n",
        "ranked_users_by_total_spend.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91c9ZsjL_btQ",
        "outputId": "3b8f72d3-f2aa-46b8-c36d-b6ce054ee1eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+----+\n",
            "|user_id|sum(amount)|rank|\n",
            "+-------+-----------+----+\n",
            "|   U001|      26997|   1|\n",
            "|   U002|       8999|   2|\n",
            "|   U004|       7999|   3|\n",
            "|   U003|          0|   4|\n",
            "+-------+-----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LGL2B0TX_tiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3605acda"
      },
      "source": [
        "### Comparing `GroupBy` and `Window` Outputs\n",
        "\n",
        "Both `groupBy` and `Window` functions are powerful tools in PySpark for data aggregation and analysis, but they serve different purposes and produce distinct outputs.\n",
        "\n",
        "#### 1. `GroupBy` Output: `total_spend_per_user`\n",
        "\n",
        "- **Purpose**: `GroupBy` operations are used for **aggregation**, where you want to collapse multiple rows into a single summary row based on one or more grouping keys. It answers questions like 'What is the total spend for each user?' or 'How many items did each category sell?'\n",
        "- **Output Characteristics**: The output of a `groupBy` operation typically has fewer rows than the input DataFrame, as it aggregates data based on the unique values of the grouping keys. For each group, it provides a single aggregated value (e.g., sum, count, average).\n",
        "- **Example Output (`total_spend_per_user`):**\n",
        "```\n",
        "+-------+-----------+\n",
        "|user_id|sum(amount)|\n",
        "+-------+-----------+\n",
        "|   U002|       8999|\n",
        "|   U001|      26997|\n",
        "|   U004|       7999|\n",
        "|   U003|          0|\n",
        "+-------+-----------+\n",
        "```\n",
        "Here, you get one row per `user_id` showing their total spend.\n",
        "\n",
        "#### 2. `Window` Output: `running_spend_per_user` and `ranked_users_by_total_spend`\n",
        "\n",
        "- **Purpose**: `Window` functions perform calculations across a set of DataFrame rows that are related to the current row. Unlike `groupBy`, `Window` functions **do not collapse rows**. Instead, they add new columns to the DataFrame, providing context-sensitive calculations (like running totals, moving averages, or rankings) for each original row.\n",
        "- **Output Characteristics**: The output of a `Window` function typically has the **same number of rows** as the input DataFrame. It adds one or more new columns containing the results of the window calculation for each row.\n",
        "\n",
        "##### a. Running Spend per User (`running_spend_per_user`)\n",
        "\n",
        "- **Type**: Cumulative aggregate within a partition.\n",
        "- **Output Characteristics**: For each transaction, it shows the cumulative spend up to that point for that specific user.\n",
        "- **Example Output (`running_spend_per_user`):**\n",
        "```\n",
        "+-------+----------+------+-------------+\n",
        "|user_id|      date|amount|running_spend|\n",
        "+-------+----------+------+-------------+\n",
        "|   U001|2024-01-05|  9999|         9999|\n",
        "|   U001|2024-01-10| 14999|        24998|\n",
        "|   U001|2024-01-15|  1999|        26997|\n",
        "|   U002|2024-01-06|  8999|         8999|\n",
        "|   U003|2024-01-07|     0|            0|\n",
        "|   U004|2024-01-08|  7999|         7999|\n",
        "+-------+----------+------+-------------+\n",
        "```\n",
        "Here, each row of the original `df` is preserved, and a new column `running_spend` is added, showing the sum of `amount` up to that date for each user.\n",
        "\n",
        "##### b. Ranked Users by Total Spend (`ranked_users_by_total_spend`)\n",
        "\n",
        "- **Type**: Ranking function applied to an aggregated DataFrame.\n",
        "- **Output Characteristics**: It assigns a rank to each user based on their total spend. It operates on the `total_spend_per_user` (an aggregated DataFrame) and adds a rank column, keeping one row per user.\n",
        "- **Example Output (`ranked_users_by_total_spend`):**\n",
        "```\n",
        "+-------+-----------+----+\n",
        "|user_id|sum(amount)|rank|\n",
        "+-------+-----------+----+\n",
        "|   U001|      26997|   1|\n",
        "|   U002|       8999|   2|\n",
        "|   U004|       7999|   3|\n",
        "|   U003|          0|   4|\n",
        "+-------+-----------+----+\n",
        "```\n",
        "This output shows the ranking of users based on the total spend, which was initially computed using `groupBy`.\n",
        "\n",
        "#### Key Differences Summarized:\n",
        "\n",
        "| Feature         | `GroupBy`                               | `Window` (e.g., `running_spend`)                  | `Window` (e.g., `rank` on aggregated data)      |\n",
        "| :-------------- | :-------------------------------------- | :------------------------------------------------ | :---------------------------------------------- |\n",
        "| **Row Count**   | Reduces rows (aggregates)               | Preserves original rows                           | Preserves rows of the input (often aggregated)  |\n",
        "| **Output**      | Summarized data per group               | Adds new column(s) with contextual calculations   | Adds new column(s) with ranking                 |\n",
        "| **Use Case**    | Overall aggregates (total, count, avg)  | Running totals, moving averages, row comparisons  | Ranking within partitions or across the whole DF|\n",
        "\n",
        "In essence, `groupBy` is about summarization and reducing granularity, while `Window` functions are about enriching the existing data with new calculations that consider a defined 'window' of related rows."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6 DataSET"
      ],
      "metadata": {
        "id": "J4CLlMz-GGPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.\n",
        "dataframes_to_check = {\n",
        "    \"users_df\": users_df,\n",
        "    \"courses_df\": courses_df,\n",
        "    \"activity_df\": activity_df,\n",
        "    \"df_enrollments_processed\": df_enrollments_processed,\n",
        "    \"df_enriched\": df_enriched,\n",
        "    \"df\": df,\n",
        "    \"total_spend_per_user\": total_spend_per_user,\n",
        "    \"running_spend_per_user\": running_spend_per_user,\n",
        "    \"ranked_users_by_total_spend\": ranked_users_by_total_spend\n",
        "}\n",
        "\n",
        "print(\"--- Number of Partitions for DataFrames ---\")\n",
        "for df_name, df_obj in dataframes_to_check.items():\n",
        "    print(f\"DataFrame: {df_name}, Partitions: {df_obj.rdd.getNumPartitions()}\")\n",
        "\n",
        "#2.\n",
        "df_enrollments_repartitioned = df_enrollments_processed.repartition('course_id')\n",
        "\n",
        "print(f\"Original df_enrollments_processed partitions: {df_enrollments_processed.rdd.getNumPartitions()}\")\n",
        "print(f\"Repartitioned df_enrollments_repartitioned partitions: {df_enrollments_repartitioned.rdd.getNumPartitions()}\")\n",
        "\n",
        "#3\n",
        "df_enrollments_coalesced = df_enrollments_repartitioned.coalesce(1)\n",
        "\n",
        "print(f\"Coalesced df_enrollments_coalesced partitions: {df_enrollments_coalesced.rdd.getNumPartitions()}\")\n",
        "\n",
        "output_path = \"/tmp/enrollments_single_partition\"\n",
        "\n",
        "df_enrollments_coalesced.write.mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "print(f\"Coalesced DataFrame written to {output_path} in Parquet format.\")\n",
        ""
      ],
      "metadata": {
        "id": "a_zqG1puGG8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7 DATASET"
      ],
      "metadata": {
        "id": "AVcRHKJ7GiM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes_to_explain = {\n",
        "    \"df_clean_users\": df_clean_users,\n",
        "    \"df_courses_clean\": df_courses_clean,\n",
        "    \"activity_df\": activity_df,\n",
        "    \"df_enrollments_processed\": df_enrollments_processed,\n",
        "    \"df_enriched\": df_enriched,\n",
        "    \"df_payments\": df_payments,\n",
        "    \"total_spend_per_user\": total_spend_per_user,\n",
        "    \"running_spend_per_user\": running_spend_per_user,\n",
        "    \"ranked_users_by_total_spend\": ranked_users_by_total_spend\n",
        "}\n",
        "\n",
        "for df_name, df_obj in dataframes_to_explain.items():\n",
        "    print(f\"\\n--- Explain for DataFrame: {df_name} ---\")\n",
        "    df_obj.explain(True)"
      ],
      "metadata": {
        "id": "6-Ko4m6HGDbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bad DAG identified in `ranked_users_by_total_spend`:\n",
        "# The physical plan for `ranked_users_by_total_spend` includes an `Exchange SinglePartition` followed by a global `Sort`.\n",
        "# This means that after computing the total spend per user (which already involves a shuffle for aggregation),\n",
        "# Spark then gathers ALL the aggregated data into a single partition (`Exchange SinglePartition`) to perform a global sort (`Sort`).\n",
        "# This design choice, while correct for achieving a global rank, is highly inefficient and becomes a major bottleneck\n",
        "# for large datasets as it eliminates parallelism and forces all data processing onto a single executor."
      ],
      "metadata": {
        "id": "g6hE2teTGlPy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}